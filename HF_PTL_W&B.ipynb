{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "HF-PTL-W&B.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "interpreter": {
      "hash": "49f4f748258cf5fda4d40f50e67b0e810d4d715262b8f0fdbe03707797777fdd"
    },
    "kernelspec": {
      "display_name": "Python 3.7.9 64-bit ('wandb': pyenv)",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.9"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bcsherma/wandb_stuff/blob/main/HF_PTL_W%26B.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lVfyACCljZva"
      },
      "source": [
        "# Detect ungramattical sentences using W&B, PyTorch Lightning, and ü§ó"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Si8lwdIAr3IO"
      },
      "source": [
        "In this notebook, we are going to train a model to detect ungrammatical sentences from the CoLA dataset. To perform the classification, we will be using Pytorch Lightning to fine tune [DistilBERT](https://arxiv.org/abs/1910.01108), a transormer model from huggingface ü§ó.\n",
        "\n",
        "We'll use Weights & Biases to:\n",
        "- Version our model inputs and outputs using [W&B Artifacts](https://docs.wandb.ai/guides/artifacts), including preprocessing steps, train/validation splits, and model checkpoints\n",
        "- Log and visualize training and validation performance using [W&B's Pytorch Lightning integration](https://docs.wandb.ai/guides/integrations/lightning)\n",
        "- Visualize and explore the raw dataset using [W&B Tables](https://docs.wandb.ai/guides/data-vis)\n",
        "- Orchestrate a hyperparameter search using [W&B Sweeps](https://docs.wandb.ai/guides/sweeps)\n",
        "\n",
        "Be sure to follow the links that each run outputs to your W&B workspace. From here you will be able to see:\n",
        "- Live training metrics\n",
        "- The raw data as a W&B table, which you can sort, group, filter, and visualize\n",
        "- An awesome artifact graph showing our experiment pipeline\n",
        "- A sweep page with powerful visualizations of hyperparameter metrics and importance"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B-HbXf0bfZ6K"
      },
      "source": [
        "# Install some dependencies\n",
        "!pip install pandas torch pytorch-lightning transformers==4.1.1 -q\n",
        "!pip install --upgrade wandb -q"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MM02anoxvi6Z"
      },
      "source": [
        "# Bulk import cell\n",
        "import wandb\n",
        "import torch\n",
        "import transformers\n",
        "import pandas as pd\n",
        "import pytorch_lightning as pl"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qOplCCPxwY-e"
      },
      "source": [
        "wandb.login()\n",
        "# wandb.login(host=\"<local-instance-url>\").  # If you want to login to a self-hosted version of W&B"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Aa3a26WltxuT"
      },
      "source": [
        "project = \"bert-cola-ptl\"  # W&B project name here\n",
        "entity = None  # your W&B username or teamname here"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RYZgdpzpwY6w"
      },
      "source": [
        "\n",
        "# The COLA dataset\n",
        "We‚Äôll fine tune the model on The Corpus of Linguistic Acceptability (CoLA) dataset for single sentence classification. It‚Äôs a set of sentences labeled as grammatically correct or incorrect. It was first published in May of 2018, and is one of the tests included in the ‚ÄúGLUE Benchmark‚Äù on which models like BERT are competing."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ME58SGZQvLp2"
      },
      "source": [
        "We'll use a [reference artifact](https://docs.wandb.ai/guides/artifacts/references) to store a pointer to the source data. The advantage of this is that a) any runs that use this artifact reference will be able to trace their lineage back to the true source and b) we can use W&B to download the raw data in our code.\n",
        "\n",
        "The cell below starts a run with job type `register-data`. In the context of this run, we:\n",
        " 1. Create an artifact called `cola-raw`\n",
        " 2. Add a reference to the CoLA dataset to our `cola-raw` artifact\n",
        " 3. Log the `cola-raw` artifact to Weights & Biases.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pvvgwJf1fd4p"
      },
      "source": [
        "# Enter the context of a W&B Run object, referenceable with the 'run' variable\n",
        "with wandb.init(entity=entity, project=project, job_type=\"register-data\") as run:\n",
        "\n",
        "  # Construct a wandb.Artifact object\n",
        "  data_source = wandb.Artifact(\"cola-raw\", type=\"dataset\")\n",
        "\n",
        "  # Store a reference to the download URL of the CoLA dataset\n",
        "  data_source.add_reference(\"https://nyu-mll.github.io/CoLA/cola_public_1.1.zip\", name=\"zipfile\")\n",
        "  \n",
        "  # Log the artifact to W&B\n",
        "  run.log_artifact(data_source)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fMzY2rVjwiax"
      },
      "source": [
        "# Defining the preprocessing step\n",
        "\n",
        "The cell below defines the function `tokenize_data`, which transforms a list of sentences and a list of labels into a tuple of `torch.tensor` objects which can be consumed by the transormer model we'll be using. The 3 tensors returned are the tokenized form of the sentences, the attention masks indicating which tokens in each sentence correspond to actual words, and a tensor containing the original labels."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gg6lF5qx0HWC"
      },
      "source": [
        "def tokenize_data(sentences, labels):\n",
        "\n",
        "  # Tokenize all of the sentences and map the tokens to thier word IDs.\n",
        "  input_ids = []\n",
        "  attention_masks = []\n",
        "\n",
        "  # Get BertTokenizer from transformers\n",
        "  tokenizer = transformers.BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
        "\n",
        "  # For every sentence...\n",
        "  for sent in sentences:\n",
        "    \n",
        "    # `encode_plus` will:\n",
        "    #   (1) Tokenize the sentence.\n",
        "    #   (2) Prepend the `[CLS]` token to the start.\n",
        "    #   (3) Append the `[SEP]` token to the end.\n",
        "    #   (4) Map tokens to their IDs.\n",
        "    #   (5) Pad or truncate the sentence to `max_length`\n",
        "    #   (6) Create attention masks for [PAD] tokens.\n",
        "    encoded_dict = tokenizer.encode_plus(\n",
        "                      sent,                      # Sentence to encode.\n",
        "                      add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
        "                      max_length = 64,           # Pad & truncate all sentences.\n",
        "                      pad_to_max_length = True,\n",
        "                      return_attention_mask = True,   # Construct attn. masks.\n",
        "                      return_tensors = 'pt',     # Return pytorch tensors.\n",
        "                   )\n",
        "    \n",
        "    # Add the encoded sentence to the list.\n",
        "    input_ids.append(encoded_dict['input_ids'])\n",
        "    \n",
        "    # And its attention mask (simply differentiates padding from non-padding).\n",
        "    attention_masks.append(encoded_dict['attention_mask'])\n",
        "\n",
        "  # Convert the lists into tensors.\n",
        "  input_ids = torch.cat(input_ids, dim=0)\n",
        "  attention_masks = torch.cat(attention_masks, dim=0)\n",
        "  labels = torch.tensor(labels)\n",
        "  return input_ids, attention_masks, labels"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jJiRZjLVxnPt"
      },
      "source": [
        "# Tokenization\n",
        "\n",
        "The code below executes a run of type `preprocess-data`, which will\n",
        "1. Download the CoLA dataset using the reference artifact we logged previously\n",
        "2. Log the entire dataset to W&B as a Table\n",
        "3. Use the function `tokenize_data` to transform the sentences in the raw data into `BERT`-consumable features.\n",
        "4. Log the preprocessed data as an artifact to W&B."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "22Xgldbkfmg-"
      },
      "source": [
        "with wandb.init(entity=entity, project=project, job_type=\"preprocess-data\") as run:\n",
        "  \n",
        "  # Download the raw cola data from the 'zipfile' reference we added to the cola-raw artifact.\n",
        "  raw_data_artifact = run.use_artifact(\"cola-raw:latest\")\n",
        "  zip_path = raw_data_artifact.get_path(\"zipfile\").download()\n",
        "  !unzip -o $zip_path  # jupyter hack to unzip data :P\n",
        "  \n",
        "  # Read in the raw data, log it to W&B as a wandb.Table\n",
        "  df = pd.read_csv(\n",
        "    \"./cola_public/raw/in_domain_train.tsv\", \n",
        "    delimiter='\\t', \n",
        "    header=None, \n",
        "    names=['sentence_source', 'label', 'label_notes', 'sentence']\n",
        "  )\n",
        "  run.log({\"raw-data\": wandb.Table(dataframe=df)})\n",
        "  \n",
        "  # Perform tokenization and store as a TensorDataset\n",
        "  input_ids, attention_masks, labels = tokenize_data(df.sentence.values, df.label.values)\n",
        "  preprocessed_data = torch.utils.data.TensorDataset(input_ids, attention_masks, labels)\n",
        "  \n",
        "  # 1. Create an artifact called preprocessed-data\n",
        "  # 2. Save the dataset to a local fil called preprocessed-data.pt\n",
        "  # 3. Add that file to the preprocessed-data artifact\n",
        "  # 4. Log the artifact to W&B\n",
        "  data_artifact = wandb.Artifact(\"preprocessed-data\", type=\"dataset\")\n",
        "  with open(\"preprocessed-data.pt\", \"wb\") as f:\n",
        "    torch.save(preprocessed_data, f)\n",
        "  data_artifact.add_file(\"preprocessed-data.pt\", name=\"dataset\")\n",
        "  run.log_artifact(data_artifact)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OKLJqruxwcHT"
      },
      "source": [
        "# Splitting the data\n",
        "\n",
        "For our training process, we want to split the data into a train and validation set. The train set is the data we will use to update the model parameters, while the validation set will be a smaller segment of data that we use to test whether our model is generalizing to examples that it hasn't been trained on.\n",
        "\n",
        "The cell below executes a `wandb.Run` with `job_type=\"split-data\"`. In the context of this run we will:\n",
        "\n",
        "1. Download the `preprocessed-data` artifact logged by our previous run\n",
        "2. Use the `random_split` function from `torch` to perform a randomn 90/10 test/valiation split on the preprocessed data\n",
        "3. Store the split datasets in a new artifact called `split-dataset`\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B0gBncWd6Nl5"
      },
      "source": [
        "with wandb.init(entity=entity, project=project, job_type=\"split-data\") as run:\n",
        "\n",
        "  # Download the preprocessed data\n",
        "  pp_data_artifact = run.use_artifact(\"preprocessed-data:latest\")\n",
        "  data_path = pp_data_artifact.get_path(\"dataset\").download()\n",
        "  dataset = torch.load(data_path)\n",
        "  \n",
        "\n",
        "  # Calculate the number of samples to include in each set.\n",
        "  train_size = int(0.9 * len(dataset))\n",
        "  val_size = len(dataset) - train_size\n",
        "\n",
        "  # Divide the dataset by randomly selecting samples.\n",
        "  train_dataset, val_dataset = torch.utils.data.random_split(dataset, [train_size, val_size])\n",
        "\n",
        "  # Construct a new artifact\n",
        "  split_data_artifact = wandb.Artifact(\"split-dataset\", type=\"dataset\")\n",
        "  \n",
        "  # Save the dataset splits to disk\n",
        "  torch.save(train_dataset, \"train.pt\")\n",
        "  torch.save(val_dataset, \"validation.pt\")\n",
        "  \n",
        "  # Add the data splits to the artifact\n",
        "  split_data_artifact.add_file(\"train.pt\", name=\"train-data\")\n",
        "  split_data_artifact.add_file(\"validation.pt\", name=\"validation-data\")\n",
        "  \n",
        "  # Log the artifact to W&B\n",
        "  run.log_artifact(split_data_artifact)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T9j7QQ-h02t6"
      },
      "source": [
        "# Defining our model\n",
        "\n",
        "We define our model and the associated training + validation procedures in the `LightningModule` below. The model itself is a pre-trained `DistilBertForSequenceClassification` with two labels."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WJ31ixyKLR3N"
      },
      "source": [
        "class SentenceClassifier(pl.LightningModule):\n",
        "  \n",
        "  def __init__(self, learning_rate=5e-5):\n",
        "    super(SentenceClassifier, self).__init__()\n",
        "    \n",
        "    # Load pretrained distilbert-base-uncased configured for classification with 2 labels\n",
        "    self.model = transformers.DistilBertForSequenceClassification.from_pretrained(\n",
        "      \"distilbert-base-uncased\", \n",
        "      num_labels = 2, \n",
        "      output_attentions = False, # Whether the model returns attentions weights.\n",
        "      output_hidden_states = False, # Whether the model returns all hidden-states.\n",
        "    )\n",
        "    self.learning_rate = learning_rate\n",
        "\n",
        "  def training_step(self, batch, batch_no):\n",
        "    \"\"\"\n",
        "    This function overrides the pl.LightningModule class. \n",
        "    \n",
        "    When trainer.fit is called, each batch from the provided data loader is fed \n",
        "    to this function successively. \n",
        "    \n",
        "    The function returns a dictionary where \"loss\" points to the loss on an \n",
        "    individual batch and the other keys can be used at our discretion to \n",
        "    aggregate results and metrics.\n",
        "    \"\"\"\n",
        "    ids, masks, labels = batch\n",
        "    outputs = self.model(ids, attention_mask=masks, labels=labels)\n",
        "    return {\"loss\": outputs[\"loss\"], \"batch_size\": len(ids)}\n",
        "  \n",
        "  def training_step_end(self, output):\n",
        "    \"\"\"\n",
        "    This function is called at the end of each training step.\n",
        "\n",
        "    The output argument is used to pass the return value from training_step.\n",
        "\n",
        "    Here, we log the average batch loss with self.log and then return output,\n",
        "    passing it to training_epoch_end.\n",
        "    \"\"\"\n",
        "    self.log(\"train/batch_avg_loss\", output[\"loss\"].item()/output[\"batch_size\"])\n",
        "    return output\n",
        "\n",
        "  def training_epoch_end(self, outputs):\n",
        "    \"\"\"\n",
        "    This function is called at the end of each training epoch.\n",
        "\n",
        "    The outputs argument is used to pass all the values returned by\n",
        "    training_step_end over the course of the epoch as a list of dictionaries.\n",
        "\n",
        "    We use those outputs here to compute and log average metrics for the entire\n",
        "    epoch.\n",
        "    \"\"\"\n",
        "    total_loss = sum(o[\"loss\"].item() for o in outputs)\n",
        "    total_examples = sum(o[\"batch_size\"] for o in outputs)\n",
        "    self.log(\"train/epoch_avg_loss\", total_loss/total_examples)\n",
        "\n",
        "  def validation_step(self, batch, batch_no):\n",
        "    \"\"\"\n",
        "    The relation between validation_step, validation_step_end, and \n",
        "    validation_epoch end is the same as with the analagous training methods\n",
        "    above. The different here is that the loss key in the output is not used\n",
        "    to compute gradients or update model parameters, and we can different\n",
        "    metrics for the validation data, in this case accuracy.\n",
        "    \"\"\"\n",
        "    ids, masks, labels = batch\n",
        "    outputs = self.model(ids, attention_mask=masks, labels=labels)\n",
        "    preds = torch.argmax(outputs[\"logits\"], axis=1)\n",
        "    correct = sum(preds.flatten() == labels.flatten())\n",
        "    return {\n",
        "        \"loss\": outputs[\"loss\"], \n",
        "        \"batch_size\": len(ids),\n",
        "        \"correct\": correct,\n",
        "    }\n",
        "\n",
        "  def validation_step_end(self, output):\n",
        "    # Not interetested in recording batch metrics for validation data.\n",
        "    return output\n",
        "  \n",
        "  def validation_epoch_end(self, outputs):\n",
        "    # Here we compute average loss and accuracy on this epoch.\n",
        "    total_loss = sum(o[\"loss\"].item() for o in outputs)\n",
        "    total_correct = sum(o[\"correct\"] for o in outputs)\n",
        "    total_examples = sum(o[\"batch_size\"] for o in outputs)\n",
        "    self.log(\"validation/epoch_avg_loss\", total_loss/total_examples)\n",
        "    self.log(\"validation/epoch_acc\", total_correct/total_examples)\n",
        "\n",
        "  def configure_optimizers(self):\n",
        "    \"\"\"\n",
        "    This is overriding a LightningModule method that is called to return the\n",
        "    optimizer used for training.\n",
        "    \"\"\"\n",
        "    return transformers.AdamW(\n",
        "        self.model.parameters(),\n",
        "        lr = self.learning_rate, \n",
        "        eps = 1e-8 \n",
        "    )\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OcE-e6wAjCQQ"
      },
      "source": [
        "# Run training in the context of a W&B run\n",
        "\n",
        "In the cell below, we define a function `train` which sets up and performs training in the context of a W&B run. The train function takes a configuration dictionary as input then passes in the `wandb.init` `config` keyword argument. We use the values saved in the `wandb.config` object associated with the run to set the parameters of our trainer and data loaders. This is the best practice to ensure that the values logged in the `config` object (and displayed in the run table of the W&B app) are aligned with the actual parameters of the experiment."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aKftJ6m5B2KM"
      },
      "source": [
        "def train(config={\"learning_rate\": 5e-5, \"batch_size\": 16, \"epochs\": 2}):\n",
        "  with wandb.init(project=project, entity=entity, job_type=\"train\", config=config) as run:\n",
        "    \n",
        "\n",
        "    # Load the datasets from the split-dataset artifact\n",
        "    data = run.use_artifact(\"split-dataset:latest\")\n",
        "    train_dataset = torch.load(data.get_path(\"train-data\").download())\n",
        "    val_dataset = torch.load(data.get_path(\"validation-data\").download())\n",
        "\n",
        "    # Extract the config object associated with the run\n",
        "    config = run.config\n",
        "    \n",
        "    # Construct our LightningModule with the learning rate from the config object\n",
        "    model = SentenceClassifier(learning_rate=config.learning_rate)\n",
        "\n",
        "    # This logger is used when we call self.log inside the LightningModule\n",
        "    logger = pl.loggers.WandbLogger(experiment=run, log_model=True)\n",
        "    \n",
        "    # Use as many GPUs as are available\n",
        "    gpus = -1 if torch.cuda.is_available() else 0\n",
        "    \n",
        "    # Construct a Trainer object with the W&B logger we created and epoch set by the config object\n",
        "    trainer = pl.Trainer(max_epochs=config.epochs, gpus=gpus, logger=logger)\n",
        "    \n",
        "    # Build data loaders for our datasets, using the batch_size from our config object\n",
        "    train_data_loader = torch.utils.data.DataLoader(train_dataset, batch_size=config.batch_size)\n",
        "    val_data_loader = torch.utils.data.DataLoader(val_dataset, batch_size=config.batch_size)\n",
        "    \n",
        "    # Execute training\n",
        "    trainer.fit(model, train_data_loader, val_data_loader)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b8DjRBbLorNI"
      },
      "source": [
        "train()  # Run training with default parameters"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lw46QEkcq2fG"
      },
      "source": [
        "# Exploring hyperparameters with W&B Sweeps\n",
        "\n",
        "W&B sweeps allow you to optimize your model hyperparameters with minimal effort. The basic workflow of sweeps is to:\n",
        "1. Construct a dictionary or YAML file that defines the hyperparameter space \n",
        "2. Call `wandb.sweep(<sweep-dict>)` from the python library or `wandb sweep <yaml-file>` from the command line to initialize the sweep in W&B\n",
        "3. Run `wandb.agent(<sweep-id>)` (python lib) or `wandb agent <sweep-id>` (cli) to pull hyperparameter combinations from W&B to run training and log values back to W&B\n",
        "\n",
        "In this particular notebook, as seen below, we will:\n",
        "1. Create a `sweep_config` dictionary describing our hyperparameter space and objective\n",
        "  - The hyperparameters we will sweep over are `learning_rate`, `batch_size`, and `epochs`\n",
        "  - Our objective in this sweep is to maximize the `validation/epoch_acc` metric logged to W&B\n",
        "  - We will use the `random` strategy, which means we will sample uniformly from the parameter space indefinitely\n",
        "2. Call `wandb.sweep(sweep_config)` to create the sweep in our W&B project\n",
        "  - `wandb.sweep` will return a unique id for the sweep, saved as `sweep_id`\n",
        "3. Call `wandb.agent(sweep_id, function=train)` to create an agent that will execute training with different hyperparameter combinations\n",
        "  - The agent will repeatedly query W&B for hyperparameter combinations\n",
        "  - Each combination is passed to the agent as a dictionary mapping hyperparmameter names to values\n",
        "    - example: `{\"learning_rate\": 2e-5, \"epochs\": 3, \"batch_size\": 32}`\n",
        "  - The dictionary we get from W&B is passed to the function given as the `function` keyword argument to the agent, in this case `train`\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3r1R3eTnXLPm"
      },
      "source": [
        "sweep_config = {\n",
        "    'method': 'random',  # Randomly sample the hyperparameter space (alternatives: grid, bayes)\n",
        "    'metric': {  # This is the metric we are interested in maximizing\n",
        "      'name': 'validation/epoch_acc',\n",
        "      'goal': 'maximize'   \n",
        "    },\n",
        "    # Paramters and parameter values we are sweeping across\n",
        "    'parameters': {\n",
        "        'learning_rate': {\n",
        "            'values': [ 5e-5, 3e-5, 2e-5]\n",
        "        },\n",
        "        'batch_size': {\n",
        "            'values': [16, 32]\n",
        "        },\n",
        "        'epochs':{\n",
        "            'values':[2, 3, 4]\n",
        "        }\n",
        "    }\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5QZj6Yn_XVnd"
      },
      "source": [
        "sweep_id = wandb.sweep(sweep_config, project=project, entity=entity)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YKmIPyYzH8LS"
      },
      "source": [
        "wandb.agent(sweep_id, function=train)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}